# B.tech_final_year_project
objective: is to build a very simple(cost wise, usage) blind guider model unlike typical others.

# Application:
  This project wont help them to walk rather let them know whats going around them which is a real vision. eg: a boy riding a cycle, a man walks down the street, a man standing near a blue car.
![Screenshot from 2021-05-14 22-16-50](https://user-images.githubusercontent.com/28312002/118302873-6260b880-b502-11eb-99cc-a8f70a90569d.png)


# Technique Used:
  Soft attention based caption generation.

# Output video link: (only generating captions) partial completion
https://www.youtube.com/watch?v=4yw9_wQNB3g

# Output video link: (voice call to mobile phone) fully completed.. but not shown with real time experiment due to covid-19 situation(used a video file)
https://www.youtube.com/watch?v=0ZAyGNsJUiY

in the voice call for first 10 secs it request for subscription and then give out the info for blind.. The request can be avoided when pay for the voice calls from server side. And in that you can see.. In 3 secs of delay the voice call reached to phone, which was ran for first time after starting the program, but will come to within 1 and half sec delay from 3secs. if the blind persons phone is set to auto answer calls, he/she can hear contineously. And blind will video stream(apps were avail in playstore itself) or he himself will take pic(by fingerprint scanner tapping) holding phone in the direction he/she faces.


