{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import torch\n",
    "import imageio                           # is used as like scipy 'imread'\n",
    "import scipy\n",
    "from scipy.misc import imread,imresize\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from random import seed, choice, sample\n",
    "import string\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization for the dataset\n",
    "dataset='coco'\n",
    "json_path='coco_raw.json'\n",
    "image_folder='/home/hemanth/Documents/dataset/MS_COCO_2017'\n",
    "captions_per_image=5\n",
    "threshold_freq=5\n",
    "output_folder=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_path, 'r') as j:\n",
    "    imgs = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example processed tokens:\n",
      "before :  A man is in a kitchen making pizzas.\n",
      "after :  ['a', 'man', 'is', 'in', 'a', 'kitchen', 'making', 'pizzas.'] \n",
      "\n",
      "before :  The dining table near the kitchen has a bowl of fruit on it.\n",
      "after :  ['the', 'dining', 'table', 'near', 'the', 'kitchen', 'has', 'a', 'bowl', 'of', 'fruit', 'on', 'it.'] \n",
      "\n",
      "before :  a person with a shopping cart on a city street \n",
      "after :  ['a', 'person', 'with', 'a', 'shopping', 'cart', 'on', 'a', 'city', 'street'] \n",
      "\n",
      "before :  A person on a skateboard and bike at a skate park.\n",
      "after :  ['a', 'person', 'on', 'a', 'skateboard', 'and', 'bike', 'at', 'a', 'skate', 'park.'] \n",
      "\n",
      "before :  a blue bike parked on a side walk \n",
      "after :  ['a', 'blue', 'bike', 'parked', 'on', 'a', 'side', 'walk'] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'file_path': '/home/hemanth/Documents/dataset/MS_COCO_2017/val2017/000000397133.jpg', 'id': 397133, 'captions': ['A man is in a kitchen making pizzas.', 'Man in apron standing on front of oven with pans and bakeware', 'A baker is working in the kitchen rolling dough.', 'A person standing by a stove in a kitchen.', 'A table with pies being made and a person standing near a wall with pots and pans hanging on the wall.'], 'processed_tokens': [['a', 'man', 'is', 'in', 'a', 'kitchen', 'making', 'pizzas.'], ['man', 'in', 'apron', 'standing', 'on', 'front', 'of', 'oven', 'with', 'pans', 'and', 'bakeware'], ['a', 'baker', 'is', 'working', 'in', 'the', 'kitchen', 'rolling', 'dough.'], ['a', 'person', 'standing', 'by', 'a', 'stove', 'in', 'a', 'kitchen.'], ['a', 'table', 'with', 'pies', 'being', 'made', 'and', 'a', 'person', 'standing', 'near', 'a', 'wall', 'with', 'pots', 'and', 'pans', 'hanging', 'on', 'the', 'wall.']]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepro_captions(imgs):\n",
    "    # preprocess all the caption\n",
    "    print('example processed tokens:')\n",
    "    for i,img in enumerate(imgs):\n",
    "        img['processed_tokens'] = []\n",
    "        for j,s in enumerate(img['captions']):\n",
    "            if i < 5 and j == 0: print('before : ',str(s))\n",
    "            txt = str(s).lower().translate(string.punctuation).strip(\"\").split()    #translate(None, string.punctuation)\n",
    "            img['processed_tokens'].append(txt)\n",
    "            if i < 5 and j == 0: print('after : ',txt,'\\n')\n",
    "    print(\"\\n\\n\")\n",
    "# imgs ={ {filepath:, id: ,captions:{},processed_tokens:[]}, {filepath:, id: ,captions:{},processed_tokens:[]}, ....}\n",
    "    print(imgs[0],'\\n')\n",
    "\n",
    "# tokenization and preprocessing \n",
    "prepro_captions(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 118287 val : 5000\n",
      "\n",
      "Actual Total no of captions: 616435\n"
     ]
    }
   ],
   "source": [
    "t=0\n",
    "v=0\n",
    "for i,img in enumerate(imgs):\n",
    "    if img['file_path'][45] == 't':\n",
    "        t=t+1\n",
    "    else:\n",
    "        v=v+1\n",
    "print(\"train : %d\"%t, \"val : %d\"%v)\n",
    "print(\"\\nActual Total no of captions:\",5*(t+v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated & processed total no of captions :  616435\n",
      "max length sentence in raw data :  50\n",
      "\n",
      "sentence length distribution (count, number of words):\n",
      " 0:          0   0.000000%\n",
      " 1:          0   0.000000%\n",
      " 2:          0   0.000000%\n",
      " 3:          0   0.000000%\n",
      " 4:          0   0.000000%\n",
      " 5:          1   0.000162%\n",
      " 6:         13   0.002109%\n",
      " 7:       4797   0.778184%\n",
      " 8:     102041   16.553408%\n",
      " 9:     134485   21.816574%\n",
      "10:     132520   21.497806%\n",
      "11:      95052   15.419631%\n",
      "12:      60408   9.799573%\n",
      "13:      35064   5.688191%\n",
      "14:      19947   3.235864%\n",
      "15:      11405   1.850155%\n",
      "16:       6846   1.110579%\n",
      "17:       4274   0.693342%\n",
      "18:       2733   0.443356%\n",
      "19:       1899   0.308062%\n",
      "20:       1301   0.211052%\n",
      "21:        910   0.147623%\n",
      "22:        665   0.107878%\n",
      "23:        500   0.081112%\n",
      "24:        326   0.052885%\n",
      "25:        261   0.042340%\n",
      "26:        180   0.029200%\n",
      "27:        164   0.026605%\n",
      "28:         88   0.014276%\n",
      "29:         71   0.011518%\n",
      "30:         60   0.009733%\n",
      "31:         57   0.009247%\n",
      "32:         43   0.006976%\n",
      "33:         46   0.007462%\n",
      "34:         46   0.007462%\n",
      "35:         31   0.005029%\n",
      "36:         20   0.003244%\n",
      "37:         23   0.003731%\n",
      "38:         21   0.003407%\n",
      "39:         23   0.003731%\n",
      "40:         22   0.003569%\n",
      "41:         16   0.002596%\n",
      "42:         14   0.002271%\n",
      "43:         21   0.003407%\n",
      "44:         14   0.002271%\n",
      "45:          8   0.001298%\n",
      "46:          6   0.000973%\n",
      "47:          8   0.001298%\n",
      "48:          1   0.000162%\n",
      "49:          3   0.000487%\n",
      "50:          1   0.000162%\n",
      "\n",
      "Word Frequencies were counted successfully..!!\n"
     ]
    }
   ],
   "source": [
    "# max length?\n",
    "# word frequency?\n",
    "sent_lengths = {}\n",
    "word_freq=Counter()\n",
    "totcap=0\n",
    "for img in imgs:\n",
    "    for txt in img['processed_tokens']:\n",
    "        totcap=totcap+1\n",
    "        word_freq.update(txt)\n",
    "        nw = len(txt)\n",
    "        sent_lengths[nw] = sent_lengths.get(nw, 0) + 1\n",
    "\n",
    "max_len = max(sent_lengths.keys())\n",
    "print('Calculated & processed total no of captions :  %d' % totcap)\n",
    "print('max length sentence in raw data :  %d\\n' % max_len)\n",
    "print('sentence length distribution (count, number of words):')\n",
    "sum_len = sum(sent_lengths.values())\n",
    "for i in range(max_len+1):\n",
    "    print('%2d: %10d   %f%%' % (i, sent_lengths.get(i,0), sent_lengths.get(i,0)*100.0/sum_len))\n",
    "print(\"\\nWord Frequencies were counted successfully..!!\")\n",
    "#print('\\nWord frequencies:\\n\\n',word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Word - Index\" mapping is done!!..\n",
      "\n",
      "Vocab size:  13027\n"
     ]
    }
   ],
   "source": [
    "# thresholding the words\n",
    "# creating word map or vocab mapping  'word to index' and 'index to word'\n",
    "vocab=[w for w in list(word_freq.keys()) if word_freq[w] > threshold_freq]\n",
    "\n",
    "w2i = {k: v + 1 for v, k in enumerate(vocab)}\n",
    "w2i['<unk>'] = len(w2i) + 1\n",
    "w2i['<start>'] = len(w2i) + 1\n",
    "w2i['<end>'] = len(w2i) + 1\n",
    "w2i['<pad>'] = 0\n",
    "\n",
    "i2w = {v + 1: k for v, k in enumerate(vocab)}\n",
    "i2w[len(i2w) + 1] = '<unk>'\n",
    "i2w[len(i2w) + 1] = '<start>'\n",
    "i2w[len(i2w) + 1] = '<end>'\n",
    "i2w[0]='<pad>'\n",
    "# saving the 'w2i and i2w' dictionary by json file format\n",
    "with open('w2i_coco.json','w') as j:\n",
    "    json.dump(w2i, j)\n",
    "with open('i2w_coco.json','w') as j:\n",
    "    json.dump(i2w,j)\n",
    "print('\"Word - Index\" mapping is done!!..\\n')\n",
    "print('Vocab size: ',len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hemanth/Documents/dataset/MS_COCO_2017/val2017/000000397133.jpg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[0]['file_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123287/123287 [00:00<00:00, 1608653.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and val dataset splitting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"train and val dataset splitting...\")\n",
    "# spliting the train and val datasets seperately..  \n",
    "train_image_paths=[]\n",
    "train_image_captions=[]\n",
    "val_image_paths=[]\n",
    "val_image_captions=[]\n",
    "for img in tqdm(imgs,position=0,leave=True):\n",
    "    impath = img['file_path']\n",
    "    imcaps = img['processed_tokens']\n",
    "    if img['file_path'][45] == 't':\n",
    "        train_image_paths.append(impath)\n",
    "        train_image_captions.append(imcaps)\n",
    "    elif img['file_path'][45] == 'v':\n",
    "        val_image_paths.append(impath)\n",
    "        val_image_captions.append(imcaps)\n",
    "    else:\n",
    "        print('ERROR............in assigining dataset paths...........!!!!!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/118287 [00:00<?, ?it/s]/home/hemanth/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/home/hemanth/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:40: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  0%|          | 13/118287 [00:00<15:36, 126.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 file creation for TRAIN started...!!!\n",
      "\n",
      "Reading TRAIN images and captions, storing to file...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118287/118287 [47:26<00:00, 41.56it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HDF5 file for TRAIN created successfully.......!!!!\n",
      "\n",
      "JSON file for TRAIN is created\n",
      "\n",
      "ALL JSON file created successfully.......!!!!\n",
      "\n",
      "ALL Input files were preprocessed successfully...........!!!!!\n",
      "HDF5 file creation for VAL started...!!!\n",
      "\n",
      "Reading VAL images and captions, storing to file...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:40<00:00, 49.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HDF5 file for VAL created successfully.......!!!!\n",
      "\n",
      "JSON file for VAL is created\n",
      "\n",
      "ALL JSON file created successfully.......!!!!\n",
      "\n",
      "ALL Input files were preprocessed successfully...........!!!!!\n"
     ]
    }
   ],
   "source": [
    "base_filename = dataset + '_' + str(captions_per_image) + '_cap_per_img_' + str(threshold_freq) + '_min_word_freq'\n",
    "seed(123)\n",
    "for impaths, imcaps, split in [(train_image_paths, train_image_captions, 'TRAIN'),\n",
    "                                   (val_image_paths, val_image_captions, 'VAL')]:\n",
    "\n",
    "    # removing the same file if already existing..\n",
    "    rem_file=os.path.join(output_folder, split + '_IMAGES_' + base_filename + '.hdf5')\n",
    "    if os.path.exists(rem_file):\n",
    "        os.remove(rem_file)\n",
    "    # saving in hdf5 file\n",
    "    print(\"HDF5 file creation for %s started...!!!\"%split)\n",
    "    with h5py.File(os.path.join(output_folder, split + '_IMAGES_' + base_filename + '.hdf5'), 'a') as h:\n",
    "        h.attrs['captions_per_image'] = captions_per_image\n",
    "\n",
    "        # Create dataset inside HDF5 file to store images\n",
    "        images = h.create_dataset('images', (len(impaths), 3, 256, 256), dtype='uint8')\n",
    "\n",
    "        print(\"\\nReading %s images and captions, storing to file...\\n\" % split)\n",
    "\n",
    "        enc_captions = []\n",
    "        caplens = []\n",
    "\n",
    "        for i, path in enumerate(tqdm(impaths,position=0,leave=True)):\n",
    "\n",
    "            # Sample captions\n",
    "            if len(imcaps[i]) < captions_per_image:\n",
    "                captions = imcaps[i] + [choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]\n",
    "            else:\n",
    "                captions = sample(imcaps[i], k=captions_per_image)\n",
    "\n",
    "            # Sanity check\n",
    "            assert len(captions) == captions_per_image\n",
    "\n",
    "            # Read images\n",
    "            #from skimage import io         im = io.imread(self.file_image)\n",
    "            img = imread(impaths[i])\n",
    "            if len(img.shape) == 2:\n",
    "                img = img[:, :, np.newaxis]\n",
    "                img = np.concatenate([img, img, img], axis=2)\n",
    "            img = imresize(img, (256, 256))\n",
    "            img = img.transpose(2, 0, 1)\n",
    "            assert img.shape == (3, 256, 256)\n",
    "            assert np.max(img) <= 255\n",
    "\n",
    "            # Save image to HDF5 file\n",
    "            images[i] = img\n",
    "\n",
    "            for j, c in enumerate(captions):\n",
    "                # Encode captions\n",
    "                enc_c = [w2i['<start>']] + [w2i.get(word, w2i['<unk>']) for word in c] + [\n",
    "                        w2i['<end>']] + [w2i['<pad>']] * (max_len - len(c))\n",
    "\n",
    "                # Find caption lengths\n",
    "                c_len = len(c) + 2\n",
    "\n",
    "                enc_captions.append(enc_c)\n",
    "                caplens.append(c_len)\n",
    "\n",
    "        print(\"\\nHDF5 file for %s created successfully.......!!!!\\n\"%split)\n",
    "        \n",
    "        # Sanity check\n",
    "        assert images.shape[0] * captions_per_image == len(enc_captions) == len(caplens)\n",
    "        # Save encoded captions and their lengths to JSON files\n",
    "        with open(os.path.join(output_folder, split + '_CAPTIONS_' + base_filename + '.json'), 'w') as j:\n",
    "                json.dump(enc_captions, j)\n",
    "\n",
    "        with open(os.path.join(output_folder, split + '_CAPLENS_' + base_filename + '.json'), 'w') as j:\n",
    "                json.dump(caplens, j)\n",
    "        print(\"JSON file for %s is created\"%split)\n",
    "        \n",
    "        print(\"\\nALL JSON file created successfully.......!!!!\\n\")\n",
    "        print(\"ALL Input files were preprocessed successfully...........!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
