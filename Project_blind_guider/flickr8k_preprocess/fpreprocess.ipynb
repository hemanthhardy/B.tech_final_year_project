{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import torch\n",
    "import imageio                           # is used as like scipy 'imread'\n",
    "import scipy\n",
    "from scipy.misc import imread,imresize\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from random import seed, choice, sample\n",
    "import string\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization for the dataset\n",
    "dataset='flickr8k'\n",
    "json_path='flickr8k_raw.json'\n",
    "image_folder='/home/hemanth/Documents/dataset/flickr8k/Flickr8k_Dataset/'\n",
    "captions_per_image=5\n",
    "threshold_freq=5\n",
    "output_folder=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_path, 'r') as j:\n",
    "    imgs = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example processed tokens:\n",
      "before :  A child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "after :  ['a', 'child', 'in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry', 'way', '.'] \n",
      "\n",
      "before :  A black dog and a spotted dog are fighting\n",
      "after :  ['a', 'black', 'dog', 'and', 'a', 'spotted', 'dog', 'are', 'fighting'] \n",
      "\n",
      "before :  A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .\n",
      "after :  ['a', 'little', 'girl', 'covered', 'in', 'paint', 'sits', 'in', 'front', 'of', 'a', 'painted', 'rainbow', 'with', 'her', 'hands', 'in', 'a', 'bowl', '.'] \n",
      "\n",
      "before :  A man lays on a bench while his dog sits by him .\n",
      "after :  ['a', 'man', 'lays', 'on', 'a', 'bench', 'while', 'his', 'dog', 'sits', 'by', 'him', '.'] \n",
      "\n",
      "before :  A man in an orange hat starring at something .\n",
      "after :  ['a', 'man', 'in', 'an', 'orange', 'hat', 'starring', 'at', 'something', '.'] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'file_path': '/home/hemanth/Documents/dataset/flickr8k/Flickr8k_Dataset/train/1000268201_693b08cb0e.jpg', 'captions': ['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .'], 'processed_tokens': [['a', 'child', 'in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry', 'way', '.'], ['a', 'girl', 'going', 'into', 'a', 'wooden', 'building', '.'], ['a', 'little', 'girl', 'climbing', 'into', 'a', 'wooden', 'playhouse', '.'], ['a', 'little', 'girl', 'climbing', 'the', 'stairs', 'to', 'her', 'playhouse', '.'], ['a', 'little', 'girl', 'in', 'a', 'pink', 'dress', 'going', 'into', 'a', 'wooden', 'cabin', '.']]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepro_captions(imgs):\n",
    "    # preprocess all the caption\n",
    "    print('example processed tokens:')\n",
    "    for i,img in enumerate(imgs):\n",
    "        img['processed_tokens'] = []\n",
    "        for j,s in enumerate(img['captions']):\n",
    "            if i < 5 and j == 0: print('before : ',str(s))\n",
    "            txt = str(s).lower().translate(string.punctuation).strip(\"\").split()    #translate(None, string.punctuation)\n",
    "            img['processed_tokens'].append(txt)\n",
    "            if i < 5 and j == 0: print('after : ',txt,'\\n')\n",
    "    print(\"\\n\\n\")\n",
    "# imgs ={ {filepath:, id: ,captions:{},processed_tokens:[]}, {filepath:, id: ,captions:{},processed_tokens:[]}, ....}\n",
    "    print(imgs[0],'\\n')\n",
    "\n",
    "# tokenization and preprocessing \n",
    "prepro_captions(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 6000 val : 1000\n",
      "\n",
      "Actual Total no of captions: 35000\n"
     ]
    }
   ],
   "source": [
    "t=0\n",
    "v=0\n",
    "for i,img in enumerate(imgs):\n",
    "    if img['file_path'][58] == 't':\n",
    "        t=t+1\n",
    "    else:\n",
    "        v=v+1\n",
    "print(\"train : %d\"%t, \"val : %d\"%v)\n",
    "print(\"\\nActual Total no of captions:\",5* (t+v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated & processed total no of captions :  35000\n",
      "max length sentence in raw data :  38\n",
      "\n",
      "sentence length distribution (count, number of words):\n",
      " 0:          0   0.000000%\n",
      " 1:          0   0.000000%\n",
      " 2:          9   0.025714%\n",
      " 3:         33   0.094286%\n",
      " 4:        199   0.568571%\n",
      " 5:        471   1.345714%\n",
      " 6:       1127   3.220000%\n",
      " 7:       2328   6.651429%\n",
      " 8:       2996   8.560000%\n",
      " 9:       3340   9.542857%\n",
      "10:       3758   10.737143%\n",
      "11:       3766   10.760000%\n",
      "12:       3534   10.097143%\n",
      "13:       3144   8.982857%\n",
      "14:       2664   7.611429%\n",
      "15:       2130   6.085714%\n",
      "16:       1553   4.437143%\n",
      "17:       1141   3.260000%\n",
      "18:        823   2.351429%\n",
      "19:        636   1.817143%\n",
      "20:        434   1.240000%\n",
      "21:        288   0.822857%\n",
      "22:        163   0.465714%\n",
      "23:        161   0.460000%\n",
      "24:         99   0.282857%\n",
      "25:         64   0.182857%\n",
      "26:         53   0.151429%\n",
      "27:         26   0.074286%\n",
      "28:         22   0.062857%\n",
      "29:          8   0.022857%\n",
      "30:          9   0.025714%\n",
      "31:          9   0.025714%\n",
      "32:          5   0.014286%\n",
      "33:          3   0.008571%\n",
      "34:          2   0.005714%\n",
      "35:          1   0.002857%\n",
      "36:          0   0.000000%\n",
      "37:          0   0.000000%\n",
      "38:          1   0.002857%\n",
      "\n",
      "Word Frequencies were counted successfully..!!\n"
     ]
    }
   ],
   "source": [
    "# max length?\n",
    "# word frequency?\n",
    "sent_lengths = {}\n",
    "word_freq=Counter()\n",
    "totcap=0\n",
    "for img in imgs:\n",
    "    for txt in img['processed_tokens']:\n",
    "        totcap=totcap+1\n",
    "        word_freq.update(txt)\n",
    "        nw = len(txt)\n",
    "        sent_lengths[nw] = sent_lengths.get(nw, 0) + 1\n",
    "\n",
    "max_len = max(sent_lengths.keys())\n",
    "print('Calculated & processed total no of captions :  %d' % totcap)\n",
    "print('max length sentence in raw data :  %d\\n' % max_len)\n",
    "print('sentence length distribution (count, number of words):')\n",
    "sum_len = sum(sent_lengths.values())\n",
    "for i in range(max_len+1):\n",
    "    print('%2d: %10d   %f%%' % (i, sent_lengths.get(i,0), sent_lengths.get(i,0)*100.0/sum_len))\n",
    "print(\"\\nWord Frequencies were counted successfully..!!\")\n",
    "#print('\\nWord frequencies:\\n\\n',word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Word - Index\" mapping is done!!..\n"
     ]
    }
   ],
   "source": [
    "# thresholding the words\n",
    "# creating word map or vocab mapping  'word to index' and 'index to word'\n",
    "vocab=[w for w in list(word_freq.keys()) if word_freq[w] > threshold_freq]\n",
    "\n",
    "w2i = {k: v + 1 for v, k in enumerate(vocab)}\n",
    "w2i['<unk>'] = len(w2i) + 1\n",
    "w2i['<start>'] = len(w2i) + 1\n",
    "w2i['<end>'] = len(w2i) + 1\n",
    "w2i['<pad>'] = 0\n",
    "\n",
    "i2w = {v + 1: k for v, k in enumerate(vocab)}\n",
    "i2w[len(i2w) + 1] = '<unk>'\n",
    "i2w[len(i2w) + 1] = '<start>'\n",
    "i2w[len(i2w) + 1] = '<end>'\n",
    "i2w[0]='<pad>'\n",
    "# saving the 'w2i and i2w' dictionary by json file format\n",
    "with open('w2i_coco.json','w') as j:\n",
    "    json.dump(w2i, j)\n",
    "with open('i2w_coco.json','w') as j:\n",
    "    json.dump(i2w,j)\n",
    "print('\"Word - Index\" mapping is done!!..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hemanth/Documents/dataset/flickr8k/Flickr8k_Dataset/train/1000268201_693b08cb0e.jpg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[0]['file_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [00:00<00:00, 1311363.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and val dataset splitting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"train and val dataset splitting...\")\n",
    "# spliting the train and val datasets seperately..  \n",
    "train_image_paths=[]\n",
    "train_image_captions=[]\n",
    "val_image_paths=[]\n",
    "val_image_captions=[]\n",
    "for img in tqdm(imgs,position=0,leave=True):\n",
    "    impath = img['file_path']\n",
    "    imcaps = img['processed_tokens']\n",
    "    if img['file_path'][58] == 't':\n",
    "        train_image_paths.append(impath)\n",
    "        train_image_captions.append(imcaps)\n",
    "    elif img['file_path'][58] == 'v':\n",
    "        val_image_paths.append(impath)\n",
    "        val_image_captions.append(imcaps)\n",
    "    else:\n",
    "        print('ERROR............in assigining dataset paths...........!!!!!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6000 [00:00<?, ?it/s]/home/hemanth/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/home/hemanth/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:40: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  0%|          | 13/6000 [00:00<00:48, 123.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 file creation for TRAIN started...!!!\n",
      "\n",
      "Reading TRAIN images and captions, storing to file...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:47<00:00, 126.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HDF5 file for TRAIN created successfully.......!!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 13/1000 [00:00<00:08, 118.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file for TRAIN is created\n",
      "HDF5 file creation for VAL started...!!!\n",
      "\n",
      "Reading VAL images and captions, storing to file...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 120.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HDF5 file for VAL created successfully.......!!!!\n",
      "\n",
      "JSON file for VAL is created\n"
     ]
    }
   ],
   "source": [
    "base_filename = dataset + '_' + str(captions_per_image) + '_cap_per_img_' + str(threshold_freq) + '_min_word_freq'\n",
    "seed(123)\n",
    "for impaths, imcaps, split in [(train_image_paths, train_image_captions, 'TRAIN'),\n",
    "                                   (val_image_paths, val_image_captions, 'VAL')]:\n",
    "\n",
    "    # removing the same file if already existing..\n",
    "    rem_file=os.path.join(output_folder, split + '_IMAGES_' + base_filename + '.hdf5')\n",
    "    if os.path.exists(rem_file):\n",
    "        os.remove(rem_file)\n",
    "    # saving in hdf5 file\n",
    "    print(\"HDF5 file creation for %s started...!!!\"%split)\n",
    "    with h5py.File(os.path.join(output_folder, split + '_IMAGES_' + base_filename + '.hdf5'), 'a') as h:\n",
    "        h.attrs['captions_per_image'] = captions_per_image\n",
    "\n",
    "        # Create dataset inside HDF5 file to store images\n",
    "        images = h.create_dataset('images', (len(impaths), 3, 256, 256), dtype='uint8')\n",
    "\n",
    "        print(\"\\nReading %s images and captions, storing to file...\\n\" % split)\n",
    "\n",
    "        enc_captions = []\n",
    "        caplens = []\n",
    "\n",
    "        for i, path in enumerate(tqdm(impaths,position=0,leave=True)):\n",
    "\n",
    "            # Sample captions\n",
    "            if len(imcaps[i]) < captions_per_image:\n",
    "                captions = imcaps[i] + [choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]\n",
    "            else:\n",
    "                captions = sample(imcaps[i], k=captions_per_image)\n",
    "\n",
    "            # Sanity check\n",
    "            assert len(captions) == captions_per_image\n",
    "\n",
    "            # Read images\n",
    "            #from skimage import io         im = io.imread(self.file_image)\n",
    "            img = imread(impaths[i])\n",
    "            if len(img.shape) == 2:\n",
    "                img = img[:, :, np.newaxis]\n",
    "                img = np.concatenate([img, img, img], axis=2)\n",
    "            img = imresize(img, (256, 256))\n",
    "            img = img.transpose(2, 0, 1)\n",
    "            assert img.shape == (3, 256, 256)\n",
    "            assert np.max(img) <= 255\n",
    "\n",
    "            # Save image to HDF5 file\n",
    "            images[i] = img\n",
    "\n",
    "            for j, c in enumerate(captions):\n",
    "                # Encode captions\n",
    "                enc_c = [w2i['<start>']] + [w2i.get(word, w2i['<unk>']) for word in c] + [\n",
    "                        w2i['<end>']] + [w2i['<pad>']] * (max_len - len(c))\n",
    "\n",
    "                # Find caption lengths\n",
    "                c_len = len(c) + 2\n",
    "\n",
    "                enc_captions.append(enc_c)\n",
    "                caplens.append(c_len)\n",
    "\n",
    "        print(\"\\nHDF5 file for %s created successfully.......!!!!\\n\"%split)\n",
    "        # Sanity check\n",
    "        assert images.shape[0] * captions_per_image == len(enc_captions) == len(caplens)\n",
    "        # Save encoded captions and their lengths to JSON files\n",
    "        with open(os.path.join(output_folder, split + '_CAPTIONS_' + base_filename + '.json'), 'w') as j:\n",
    "                json.dump(enc_captions, j)\n",
    "\n",
    "        with open(os.path.join(output_folder, split + '_CAPLENS_' + base_filename + '.json'), 'w') as j:\n",
    "                json.dump(caplens, j)\n",
    "        print(\"JSON file for %s is created\"%split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JSON file created successfully.......!!!!\n",
      "\n",
      "Input files were preprocessed successfully...........!!!!!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nJSON file created successfully.......!!!!\\n\")\n",
    "print(\"Input files were preprocessed successfully...........!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
